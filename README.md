# docs_cls
В данном репозитории решается задача мульти-лейбл классификации для коллекции текстов. 
Мульти-лейбл классификация позволяет относить одному тексту сразу несколько классов. 

Для решения поставленной задачи были выбраны следующие алгоритмы:
- **tf-idf** для векторного представления входных текстов,
- **logistic regression**, обернутая в MultiOutputClassifier (так как logistic regression из библиотеки sklearn не подразумевает решение задачи мульти-ЛЕЙБЛ классификации),
- **decision tree**
- **BERT модель** 'distilbert-base-uncased' с архитектурой AutoModelForSequenceClassification с AutoTokenizer для векторного представления.

-----------------------
Входная выборка была очень разбалансирована по количеству документов для темы. Поэтому в рамках данного эксперимента, было принято решение сбалансировать выборку, путем убирания лишних документов (по теме с минимальным числом документов).

При использовании векторизации с помощью **tf-idf**, тексты подвергались предобработке (`data_processing/data_processing.ipynb`).

Для предобработки тексты были приведены к нижнему регистру, оттуда были исключены *нетекстовые* данные (email, ссылки, non-ASCII characters, числа с точками, номера телефонов), год и страна были заменены словами YEARNUM и COUNTRYNAME, исключены stop-слова, и была произведена лемматизация. 

После этого тексты были разделены на три выборки: тренировочную (70%), валидационную (10%), тестовую (20%). Выделение валидационной выборки требовалось только для нейросетевого подхода, так как в алгоритмах и sklearn, валидационная выборка извлекается из тренировочной самостоятельно. Для тестирования логистической регрессии и decision tree тестовая выборка была объединена с валидационной. 

Предобработанные тексты лежат в архиве [data_processed](https://drive.google.com/file/d/1rJCdhztSVtjv6_NO5kL9-YOxqT5X4IfI/view?usp=drive_link) плюс там же находятся тренировочный, валидационный и тестовый датасеты.

Значения классов были представлены как *one-hot* вектор. 

-----------------------
В файле `train_models.ipynb` производится тренировка алгоритмов с их последующим сохранением. 

Также, помимо tf-idf, была предпринята попытка взять векторное представление от DistilBert модели. Однако, из-за ограничений по оперативной памяти, это сделать не получись. Код для получения такого векторного представления лежит в ноутбуке.

Тренировочный скрипт для BERT модели можно найти здесь: `model_training/train_nn.ipynb`

-----------------------
В файле `model_testing/test_models.ipynb` производится тестирование logistic regression и decision tree. 

Для оценки работы мульти-лейб классификатора можно использовать следующие метрики:
- **Exact Match Ratio** - считает долю правильно предсказанных случаев, при условии, что частично-верно предсказанные примеры рассматриваются как неправильные (оно же может классифицироваться как Accuracy)
- 0/1 Loss - считает долю неправильно предсказанных случаев
- **Hamming Loss** - считает сколько раз в среднем пример был неверно классифицирован. При подсчете используется как неправильное предсказание, так и пропуск предсказания, нормализованные по общему числу примеров и классов. (Чем ниже hamming loss, тем лучше работает модель)
- **Recall** - доля правильно предсказанных примеров к общему числу истинных значений, усредненная для всех значений
- **Precision** - доля правильно предсказанных примеров к общему числу предсказанных примеров, усредненная для всех значений
- **F1-Measure** - гармоническое среднее для Precision и Recall.

Для задач мульти-лейб классификации для метрик из sklearn параметр average указан как `samples`, 
```
Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score).
```
Я думаю, что нет смысла использовать `average='samples'`, потому что в нашей тестовой выборке нет объектов, которые могут относиться сразу нескольким классам. 

Вместо этого будем использовать `average='micro'`, так как наша выборка сбалансированная (в таком случае рекомендуется использовать именно ее).

Также, для анализа результатов для нашей выборки полезно посмотреть поклассово на метрики precision, recall и f1 (такие таблицы сохраняются в папку results).

Таким образом, для **logistic regression** были получены следующие метрики:
```
Exact Match Ratio: 0.5030045434559578
Hamming loss: 0.02816209878352631

Recall micro: 0.5083540964385168
Precision micro: 0.5137036494210758
F1 Measure micro: 0.6459043582419608
```
Для **decision tree**:
```
Exact Match Ratio: 0.7394840979041477
Hamming loss: 0.018800381064048074

Recall micro: 0.763837998925204
Precision micro: 0.7893155503444232
F1 Measure micro: 0.8076331871180595
```

**BERT модель** продемонстрировала следующие результаты (`test_nn.ipynb`):
```
Exact Match Ratio: 0.7224094972885827
Hamming loss: 0.02146782940055694

Recall micro: 0.7262201377693097
Precision micro: 0.7300307782500366
F1 Measure micro: 0.7727572431447077
```
Полагаю, что BERT показал результыт хуже, чем деревья, потому что было взято всего 128 токенов, тогда как для деревьев текст рассматривался целиком. 
Можно было бы брать BERT вектора длиннее (если бы было достаточное число ресурсов), тогда качество было бы, вероятно, лучше. 

Обученные модели можно найти [здесь](https://drive.google.com/file/d/1xSBYfqceerK6PvmTAN0pmRFZCTNthnmB/view?usp=drive_link).

---------------------------------

В файле `model_testing/error_analysis.ipynb` производится поклассовый анализ ошибок для наилучшей модели. Анализ производится, чтобы посмотреть, какие классы путаются между собой, а какие классы предсказываются вместе с истинным. 

Как можно видеть на графиках, общая проблема данного алгоритма заключается в предсказывании "default" класса (то есть отсутствие предсказания). Чтобы решить данную проблему можно пересобрать тренировочную выборку, чтобы тексты максимально охватывали семантику данной предметной области.

----------------------------------

Лучше всего из 3-х реализованных алгоритмов себя показал `Decision Tree` с векторизацией tf-idf.

--------------------------------

Также был добавлен файл `infer.ipynb` для получения предсказаний от модели на выбор (logistic regression или decision tree).

---------------------------------

**Возможные пути улучшения**:
- Использовать векторное представление от BERT-модели, т.к. BERT должен лучше кодировать семантику предложения, нежели tf-idf. В этом случае, можно получить векторное представление отдельно для каждого предложения и потом взять среднее и получить вектор для текста. *(не хватило вычислительных ресурсов для реализации)*
- Использовать векторное представление от Sbert `https://www.sbert.net/` - делить текст на предложения, затем брать средний вектор для представления текста. 
- Поэксперементировать с различными предобученными BERT-моделями. *(не хватило вычислительных ресурсов для реализации)*
- Произвести лучшую процедуру предобработки входного текста, как для tf-df, так и перед подачей в BERT (имеется в виду исключить совсем неинформативные данные).
- Использовать нейросетевой подход, но на большем числе данных - различными способами добавить тексты к недостающим классам (можно их автоматически нагенерировать).
- Добавить в тренировочные и тестовые данные мульти-лейбл примеры (т.к модель все-таки решает задачу мульти-лейбл классификации).
- Производить более детальный анализ ошибок
